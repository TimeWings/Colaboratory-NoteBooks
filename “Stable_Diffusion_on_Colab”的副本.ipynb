{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zDpCHib5o48i",
        "9woTLsihHvPL",
        "FESdr0SFq6yM",
        "bHw1XtAsrF2j",
        "FxV4KJF4ouJW",
        "FpEY0oYSp47z",
        "qrAkSMlHqFLd",
        "-UDhfoj_p_Mv",
        "hpTVwndLqQ4X",
        "-e8MY5SoqX3u",
        "QVLTsPxLpAHD",
        "_Re8zckxD4O2",
        "Z-0JRk2yD1CH",
        "TTJqduQ_EBZU",
        "cN-uLLBEEGvU",
        "WY3mrjfBJ4K-",
        "jS2j9vRWKGEE",
        "an2-gonRKHPf",
        "Wzupc6zkKF5y",
        "RSpmqvb0KF1M",
        "aQObuZkFKFsB",
        "uWOV8pGF9g6f",
        "51Wca-v_9t9K",
        "TvtAnKJU9thB",
        "HswzT1-J9tZn",
        "u1lfVBO69tTL",
        "wDFl3Ndu9tHs"
      ],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimeWings/Colaboratory-NoteBooks/blob/main/%E2%80%9CStable_Diffusion_on_Colab%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the RAM or VRAM usage is too high or you want to use a different mode, click here to refresh the colab session"
      ],
      "metadata": {
        "id": "Yn-YX0pmP3Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "GrKJyOOUP30N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/pytorch"
      ],
      "metadata": {
        "id": "iX2Qs1O3we1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -r \"def randn(\" pytorch/"
      ],
      "metadata": {
        "id": "KArOjoCrwg1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Start Here**\n",
        "---\n"
      ],
      "metadata": {
        "id": "zDpCHib5o48i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model to Drive\n",
        "\n",
        "This attempts to download the model to the drive, try it if you don't have it."
      ],
      "metadata": {
        "id": "9woTLsihHvPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is experimental, but it'll try to use the torrent to download the model.\n",
        "Whole thing is copied from here: https://colab.research.google.com/github/FKLC/Torrent-To-Google-Drive-Downloader/blob/master/Torrent_To_Google_Drive_Downloader.ipynb"
      ],
      "metadata": {
        "id": "PpkkUB3bH0VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "te1CftB9H8Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install libtorrent"
      ],
      "metadata": {
        "id": "jnEA4kndJ2ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import libtorrent as lt\n",
        "\n",
        "ses = lt.session()\n",
        "ses.listen_on(6881, 6891)\n",
        "\n",
        "magnet_link = \"magnet:?xt=urn:btih:49cdb19ae5f1e64697c931b28a0b22ef98bc2484&dn=sd-v1-3-full-ema.ckpt&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2fopen.tracker.cl%3a1337%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2fopen.stealth.si%3a80%2fannounce&tr=udp%3a%2f%2fexodus.desync.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.bitsearch.to%3a1337%2fannounce&tr=udp%3a%2f%2fopen.demonii.com%3a1337%2fannounce&tr=udp%3a%2f%2fexplodie.org%3a6969%2fannounce&tr=udp%3a%2f%2fchouchou.top%3a8080%2fannounce&tr=udp%3a%2f%2fbt.oiyo.tk%3a6969%2fannounce&tr=https%3a%2f%2ftracker.nanoha.org%3a443%2fannounce&tr=https%3a%2f%2ftracker.lilithraws.org%3a443%2fannounce\"\n",
        "params = {\"save_path\": \"/content/gdrive/My Drive/\"}\n",
        "downloads = [lt.add_magnet_uri(ses, magnet_link, params)]"
      ],
      "metadata": {
        "id": "oEnZwr_NH8hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "state_str = [\n",
        "    \"queued\",\n",
        "    \"checking\",\n",
        "    \"downloading metadata\",\n",
        "    \"downloading\",\n",
        "    \"finished\",\n",
        "    \"seeding\",\n",
        "    \"allocating\",\n",
        "    \"checking fastresume\",\n",
        "]\n",
        "\n",
        "layout = widgets.Layout(width=\"auto\")\n",
        "style = {\"description_width\": \"initial\"}\n",
        "download_bars = [\n",
        "    widgets.FloatSlider(\n",
        "        step=0.01, disabled=True, layout=layout, style=style\n",
        "    )\n",
        "    for _ in downloads\n",
        "]\n",
        "display(*download_bars)\n",
        "\n",
        "while downloads:\n",
        "    next_shift = 0\n",
        "    for index, download in enumerate(downloads[:]):\n",
        "        bar = download_bars[index + next_shift]\n",
        "        if not download.is_seed():\n",
        "            s = download.status()\n",
        "\n",
        "            bar.description = \" \".join(\n",
        "                [\n",
        "                    download.name(),\n",
        "                    str(s.download_rate / 1000),\n",
        "                    \"kB/s\",\n",
        "                    state_str[s.state],\n",
        "                ]\n",
        "            )\n",
        "            bar.value = s.progress * 100\n",
        "        else:\n",
        "            next_shift -= 1\n",
        "            ses.remove_torrent(download)\n",
        "            downloads.remove(download)\n",
        "            bar.close() # Seems to be not working in Colab (see https://github.com/googlecolab/colabtools/issues/726#issue-486731758)\n",
        "            download_bars.remove(bar)\n",
        "            print(download.name(), \"complete\")\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "p5IjzbGXH8Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/gdrive/My Drive/sd-v1-3-full-ema.ckpt /content/gdrive/My Drive/model.ckpt"
      ],
      "metadata": {
        "id": "tZpO19x_I5QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From scratch (no repo)\n",
        "\n",
        "This is from a completely clean disk and gdrive, it assumes you have the model in your gdrive and it is named \"model.ckpt\"\n",
        "\n"
      ],
      "metadata": {
        "id": "FESdr0SFq6yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "sg7zt9wZHZDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive"
      ],
      "metadata": {
        "id": "EVxjM3zkJFYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3xIYmYmT_QJ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/basujindal/stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd stable-diffusion"
      ],
      "metadata": {
        "id": "akwkcjT7UTqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "EV4POY-_XCor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rmdir src/clip src/taming-transformers src/k-diffusion"
      ],
      "metadata": {
        "id": "zgpxgS8kyqxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install albumentations==0.4.3\n",
        "!pip install  opencv-python==4.1.2.30\n",
        "!pip install pudb==2019.2\n",
        "!pip install imageio==2.9.0\n",
        "!pip install imageio-ffmpeg==0.4.2\n",
        "#!pip install pytorch-lightning==1.4.2\n",
        "!pip install  pytorch-lightning \n",
        "!pip install omegaconf==2.1.1\n",
        "!pip install test-tube>=0.7.5\n",
        "!pip install streamlit>=0.73.1\n",
        "!pip install einops==0.3.0\n",
        "!pip install torch-fidelity==0.3.0\n",
        "!pip install transformers==4.19.2\n",
        "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git src/k-diffusion\n",
        "!pip install src/k-diffusion\n",
        "!pip install kornia"
      ],
      "metadata": {
        "id": "QDuANeUMa6AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This next code bit is *meant* to crash the colab."
      ],
      "metadata": {
        "id": "q-dzJUbjH_np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "sLU4g7SQxmJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean machine (saved repo)\n",
        "\n",
        "This assumes you have the fully setup repo on your gdrive and just connected to the colab (don't run if it just crashed)"
      ],
      "metadata": {
        "id": "bHw1XtAsrF2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Y9EBc437WDOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/stable-diffusion"
      ],
      "metadata": {
        "id": "E4_P7eyErKuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "h1KhcxRi--j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r src/clip src/taming-transformers src/k-diffusion"
      ],
      "metadata": {
        "id": "bzCnVrBxtj0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install albumentations==0.4.3\n",
        "!pip install opencv-python==4.1.2.30\n",
        "!pip install pudb==2019.2\n",
        "!pip install imageio==2.9.0\n",
        "!pip install imageio-ffmpeg==0.4.2\n",
        "#!pip install pytorch-lightning==1.4.2\n",
        "!pip install pytorch-lightning \n",
        "!pip install omegaconf==2.1.1\n",
        "!pip install test-tube>=0.7.5\n",
        "!pip install streamlit>=0.73.1\n",
        "!pip install einops==0.3.0\n",
        "!pip install torch-fidelity==0.3.0\n",
        "!pip install transformers==4.19.2\n",
        "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git src/k-diffusion\n",
        "!pip install src/k-diffusion\n",
        "!pip install kornia"
      ],
      "metadata": {
        "id": "ZGV_5H4xrOSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next code bit is *meant* to crash the colab."
      ],
      "metadata": {
        "id": "q5rqvIj5Icbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "GVW3g_iGrVmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Optimized Text 2 Image**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FxV4KJF4ouJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "FpEY0oYSp47z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/stable-diffusion"
      ],
      "metadata": {
        "id": "Dq5ZTTVHzTWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ],
      "metadata": {
        "id": "yqcUa1DSsVFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd"
      ],
      "metadata": {
        "id": "hTqhlJslsc-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(opt,prompt,grid):\n",
        "    device = 'cuda'\n",
        "    images = []\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    start_code = None\n",
        "\n",
        "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        all_samples = list()\n",
        "        for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "            for prompts in tqdm(data, desc=\"data\"):\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if opt.scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "\n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelCS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    samples_ddim = model.sample(S=opt.ddim_steps,\n",
        "                                                conditioning=c,\n",
        "                                                batch_size=opt.n_samples,\n",
        "                                                seed=opt.seed,\n",
        "                                                shape=shape,\n",
        "                                                verbose=False,\n",
        "                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                unconditional_conditioning=uc,\n",
        "                                                eta=opt.ddim_eta,\n",
        "                                                x_T=start_code)\n",
        "\n",
        "                    modelFS.to(device)\n",
        "                    for i in range(batch_size):\n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        if grid:\n",
        "                            all_samples.append(x_sample)\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        images +=[Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelFS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "\n",
        "                    del samples_ddim\n",
        "                    print(\"memory_final = \", torch.cuda.memory_allocated()/1e6)\n",
        "            if grid:\n",
        "                grid = torch.stack(all_samples, 0)\n",
        "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                grid = make_grid(grid, nrow=n_rows)\n",
        "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
        "\n",
        "            print(f'Finished!')\n",
        "            return images"
      ],
      "metadata": {
        "id": "5lFjABclyK1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Model"
      ],
      "metadata": {
        "id": "qrAkSMlHqFLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = '../model.ckpt' # this points to the model that is in your root gdrive folder\n",
        "\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li = []\n",
        "lo = []\n",
        "for key, value in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)"
      ],
      "metadata": {
        "id": "yUupXuBtsnQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Settings"
      ],
      "metadata": {
        "id": "-UDhfoj_p_Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config():\n",
        "      def __init__(self):\n",
        "        self.config = 'optimizedSD/v1-inference.yaml' # Don't change this\n",
        "        self.ckpt = ckpt # If you want to change the model location, change it on the Load movel section\n",
        "\n",
        "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
        "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
        "        self.C = 4 # Keep as is\n",
        "\n",
        "        self.seed = 10\n",
        "\n",
        "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
        "        self.H = 256 # Height, the vertical resolution\n",
        "        self.W = 256 # Width, the horizontal resolution\n",
        "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
        "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
        "\n",
        "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
        "        self.n_samples = 9 # Amount of images outputted\n",
        "        self.n_rows = 3 # How many images per row (used on grid)\n",
        "\n",
        "opt = config()\n",
        "seed_everything(opt.seed)"
      ],
      "metadata": {
        "id": "9bw5t11msgza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "config.modelUNet.params.ddim_steps = opt.ddim_steps\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "\n",
        "if opt.precision == \"autocast\":\n",
        "    model.half()\n",
        "    modelCS.half()\n",
        "    modelFS.half()"
      ],
      "metadata": {
        "id": "3jVS88k9mS1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run prompt"
      ],
      "metadata": {
        "id": "hpTVwndLqQ4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Anatomical cross section of a tree with a boney skeletal structure 1970s scientific diagram\" #@param {type:\"string\"}\n",
        "scale = 7.5 #@param {type:\"number\"}\n",
        "seed = 12 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 30 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "n_rows = 2 #@param {type:\"integer\"}\n",
        "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
        "opt.scale = scale\n",
        "opt.seed = seed\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "images = generate(opt=opt, prompt=prompt, grid=(grid==\"yes\"))"
      ],
      "metadata": {
        "id": "GdOYu4eWzn9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "-e8MY5SoqX3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you generated more than 9 images(the grid counts too), just add more code lines continuing the sequence\n",
        "\n",
        "If you chose grid, it's the first one\n",
        "\n",
        "Right click and save to download"
      ],
      "metadata": {
        "id": "5i_5610OPi3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "UavUwJGQzslr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "d_rwrBturmN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "PxZCEfDm2TVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "oz9WfK-Z2Tnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "_xoSKGXdutaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "UAgFGdzHveXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[5]"
      ],
      "metadata": {
        "id": "KcF8zEFVxN2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[6]"
      ],
      "metadata": {
        "id": "syNrZxGrxOIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[7]"
      ],
      "metadata": {
        "id": "qWUnH_5KxOYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[8]"
      ],
      "metadata": {
        "id": "eHSoXDEM1HSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[9]"
      ],
      "metadata": {
        "id": "KEWSWYtjObgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Image 2 Image**\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QVLTsPxLpAHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "_Re8zckxD4O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/stable-diffusion"
      ],
      "metadata": {
        "id": "CXrPmEEowI_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os, sys, glob\n",
        "import PIL\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ],
      "metadata": {
        "id": "oaiJGYY7pDcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image).half()\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def generate(opt,init_img,grid,prompt):\n",
        "    device = 'cuda'\n",
        "    images = []\n",
        "    all_samples = list()\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    assert os.path.isfile(init_img)\n",
        "    init_image = load_img(init_img).to(device)\n",
        "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                                  unconditional_conditioning=uc,)\n",
        "\n",
        "\n",
        "                        for i in range(batch_size):\n",
        "                            x_samples_ddim = model.decode_first_stage(samples[i].unsqueeze(0))\n",
        "                            x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                            if grid:\n",
        "                                all_samples.append(x_sample)\n",
        "                            x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                            images += [Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                        del samples\n",
        "    if grid:\n",
        "        grid = torch.stack(all_samples, 0)\n",
        "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "        grid = make_grid(grid, nrow=n_rows)\n",
        "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "        images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
        "\n",
        "    print(f'Finished!')\n",
        "    return images"
      ],
      "metadata": {
        "id": "_zpHIRmavzkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Settings and loading model"
      ],
      "metadata": {
        "id": "Z-0JRk2yD1CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config():\n",
        "      def __init__(self):\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml' # Don't change this\n",
        "        self.ckpt = '../model.ckpt' # this points to the model that is in your root gdrive folder\n",
        "\n",
        "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
        "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
        "        self.C = 4 # Keep as is\n",
        "\n",
        "        self.seed = 7777\n",
        "\n",
        "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
        "        self.H = 512 # Height, the vertical resolution\n",
        "        self.W = 512 # Width, the horizontal resolution\n",
        "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
        "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
        "        self.strength = 0.7 # How agressive it is, keep between 0.2 ~ 1.0\n",
        "\n",
        "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
        "        self.n_samples = 9 # Amount of images outputted\n",
        "        self.n_rows = 3 # How many images per row (used on grid)\n",
        "\n",
        "opt = config()\n",
        "seed_everything(opt.seed)"
      ],
      "metadata": {
        "id": "Pdl7SBzEv6C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\").half()\n",
        "sampler = DDIMSampler(model)"
      ],
      "metadata": {
        "id": "4vrgAiwZzrD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Run prompt"
      ],
      "metadata": {
        "id": "TTJqduQ_EBZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "add \"../\" before the name for images in the root gdrive(MyDrive) folder\n",
        "\n",
        "this video(not mine) shows how to upload images: https://siasky.net/_ABvKVbl9c9FDFywm4HauIr3g2EgPg2YbRE8burq8IsK-w\n",
        "\n",
        "strength sets how agressive it is when modifying the images"
      ],
      "metadata": {
        "id": "519jUkb1RRib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\" #@param {type:\"string\"}\n",
        "img = \"../img.png\" #@param {type:\"string\"}\n",
        "scale = 5 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 100 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "strength = 0.51 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "n_rows = 2 #@param {type:\"integer\"}\n",
        "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
        "opt.scale = scale\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "opt.strength = strength\n",
        "images = generate(opt, init_img=img, grid=(grid==\"yes\"), prompt=prompt)"
      ],
      "metadata": {
        "id": "j81cBoWg0TI4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "cN-uLLBEEGvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you generated more than 5 images(the grid counts too), just add more code lines continuing the sequence\n",
        "\n",
        "If you chose grid, it's the first one\n",
        "\n",
        "If your outputs are a colourful rainbow mess, you need to enlarge the input image\n",
        "\n",
        "Right click and save to download"
      ],
      "metadata": {
        "id": "zgt471U4-e0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "JZcUw_0ZAHIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "0JjnsHKQAKh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "g3ep6KrNALgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "OPdsSouaAqdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "9PMCGTfrVpC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Optimized Text 2 Image with k-diffusion**\n",
        "---\n"
      ],
      "metadata": {
        "id": "WY3mrjfBJ4K-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "jS2j9vRWKGEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/stable-diffusion"
      ],
      "metadata": {
        "id": "fYSOCX0sKLQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os, sys, glob\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "import accelerate\n",
        "import k_diffusion as K\n",
        "import torch.nn as nn\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ],
      "metadata": {
        "id": "n4k6xC_wKLI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "def generate(opt,prompt,grid):\n",
        "    accelerator = accelerate.Accelerator()\n",
        "    device = accelerator.device\n",
        "    images = []\n",
        "\n",
        "    seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes])\n",
        "    torch.manual_seed(seeds[accelerator.process_index].item())\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        all_samples = list()\n",
        "        with precision_scope(\"cuda\"):\n",
        "            for n in trange(opt.n_iter, desc=\"Sampling\", disable =not accelerator.is_main_process):\n",
        "                for prompts in tqdm(data, desc=\"data\", disable =not accelerator.is_main_process):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if opt.scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "\n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelCS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                    torch.manual_seed(opt.seed)\n",
        "                    x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0] # for GPU draw\n",
        "                    model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                    extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                    samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                    \n",
        "                    modelFS.to(device)\n",
        "                    for i in range(batch_size):\n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        x_sample = accelerator.gather(x_samples_ddim)\n",
        "                        if grid:\n",
        "                            all_samples.append(x_sample)\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        images +=[Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelFS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    del samples_ddim\n",
        "            if grid:\n",
        "                grid = torch.stack(all_samples, 0)\n",
        "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                grid = make_grid(grid, nrow=n_rows)\n",
        "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
        "\n",
        "            print(f'Finished!')\n",
        "            return images"
      ],
      "metadata": {
        "id": "hRuexkybKOqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Model"
      ],
      "metadata": {
        "id": "an2-gonRKHPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = '../model.ckpt' # this points to the model that is in your root gdrive folder\n",
        "\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li = []\n",
        "lo = []\n",
        "for key, value in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)"
      ],
      "metadata": {
        "id": "9U9NTLPDKuxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Settings"
      ],
      "metadata": {
        "id": "Wzupc6zkKF5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config():\n",
        "      def __init__(self):\n",
        "        self.config = 'optimizedSD/v1-inference.yaml' # Don't change this\n",
        "        self.ckpt = ckpt # If you want to change the model location, change it on the Load movel section\n",
        "\n",
        "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
        "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
        "        self.C = 4 # Keep as is\n",
        "\n",
        "        self.seed = 435455\n",
        "\n",
        "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
        "        self.H = 256 # Height, the vertical resolution\n",
        "        self.W = 256 # Width, the horizontal resolution\n",
        "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
        "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
        "\n",
        "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
        "        self.n_samples = 4 # Amount of images outputted\n",
        "        self.n_rows = 2 # How many images per row (used on grid)\n",
        "\n",
        "opt = config()\n",
        "seed_everything(opt.seed)"
      ],
      "metadata": {
        "id": "S5PI2jYtKwov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "config.modelUNet.params.ddim_steps = opt.ddim_steps\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "\n",
        "if opt.precision == \"autocast\":\n",
        "    model.half()\n",
        "    modelCS.half()\n",
        "    modelFS.half()\n",
        "\n",
        "model_wrap = K.external.CompVisDenoiser(model)\n",
        "sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()"
      ],
      "metadata": {
        "id": "DvF_YZUyKwh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Prompt"
      ],
      "metadata": {
        "id": "RSpmqvb0KF1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\" #@param {type:\"string\"}\n",
        "scale = 7.5 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 30 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "n_rows = 2 #@param {type:\"integer\"}\n",
        "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
        "opt.scale = scale\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "images = generate(opt=opt, prompt=prompt, grid=(grid==\"yes\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0_kIZqhiK0mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "aQObuZkFKFsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you generated more than 5 images(the grid counts too), just add more code lines continuing the sequence\n",
        "\n",
        "If you chose grid, it's the first one\n",
        "\n",
        "Right click and save to download"
      ],
      "metadata": {
        "id": "lp-qQ08IK2Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "aUB8qNcRK1kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "yuD5K0YQK1eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "6nb-QWw2K1Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "3BmTtPnLK1TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "TVPEkBb7K1PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Optimized Image 2 Image (not working)**\n",
        "---"
      ],
      "metadata": {
        "id": "uWOV8pGF9g6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "51Wca-v_9t9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/stable-diffusion"
      ],
      "metadata": {
        "id": "rjt_EA0K94oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os, sys, glob, random\n",
        "import copy\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from torch import autocast\n",
        "from itertools import islice\n",
        "from tqdm import tqdm, trange\n",
        "from omegaconf import OmegaConf\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "from pytorch_lightning import seed_everything\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "from ldm.util import instantiate_from_config"
      ],
      "metadata": {
        "id": "cA2E-cjD94g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd\n",
        "\n",
        "def load_img(path, h0=None, w0=None):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")   \n",
        "    if(h0 is None or w0 is None):\n",
        "        h, w = h0, w0\n",
        "    \n",
        "    w, h = map(lambda x: x - x % 32, (w0, h0))  # resize to integer multiple of 32\n",
        "\n",
        "    print(f\"New image size ({w}, {h})\")\n",
        "    image = image.resize((w, h), resample = Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def generate(opt,init_img,grid,prompt):\n",
        "    device = 'cuda'\n",
        "    images = []\n",
        "    all_samples = list()\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    modelFS.to(device)\n",
        "\n",
        "    assert os.path.isfile(init_img)\n",
        "    init_image = load_img(init_img, opt.H, opt.W).to(device)\n",
        "    if opt.precision == \"autocast\":\n",
        "        init_image = init_image.half()\n",
        "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "    init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    mem = torch.cuda.memory_allocated()/1e6\n",
        "    modelFS.to(\"cpu\")\n",
        "    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "        time.sleep(1)\n",
        "\n",
        "    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        all_samples = list()\n",
        "        for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "            for prompts in tqdm(data, desc=\"data\"):\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if opt.scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "\n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelCS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    z_enc = model.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                    samples = model.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                            unconditional_conditioning=uc,)\n",
        "\n",
        "                    modelFS.to(device)\n",
        "                    for i in range(batch_size):\n",
        "                        x_samples_ddim = model.decode_first_stage(samples[i].unsqueeze(0))\n",
        "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        if grid:\n",
        "                            all_samples.append(x_sample)\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        images += [Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelFS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    del samples\n",
        "    if grid:\n",
        "        grid = torch.stack(all_samples, 0)\n",
        "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "        grid = make_grid(grid, nrow=n_rows)\n",
        "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "        images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
        "\n",
        "    print(f'Finished!')\n",
        "    return images"
      ],
      "metadata": {
        "id": "KnZxeD7n94aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Model"
      ],
      "metadata": {
        "id": "TvtAnKJU9thB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = '../model.ckpt' # this points to the model that is in your root gdrive folder\n",
        "\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li = []\n",
        "lo = []\n",
        "for key, value in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)"
      ],
      "metadata": {
        "id": "35p_Wudi-H1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Settings"
      ],
      "metadata": {
        "id": "HswzT1-J9tZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config():\n",
        "      def __init__(self):\n",
        "        self.config = 'optimizedSD/v1-inference.yaml' # Don't change this\n",
        "        self.ckpt = ckpt # If you want to change the model location, change it on the Load Model section\n",
        "\n",
        "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
        "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
        "        self.C = 4 # Keep as is\n",
        "\n",
        "        self.seed = 7777\n",
        "\n",
        "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
        "        self.H = 512 # Height, the vertical resolution\n",
        "        self.W = 256 # Width, the horizontal resolution\n",
        "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
        "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
        "        self.strength = 0.7 # How agressive it is, keep between 0.2 ~ 1.0\n",
        "\n",
        "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
        "        self.n_samples = 9 # Amount of images outputted\n",
        "        self.n_rows = 3 # How many images per row (used on grid)\n",
        "\n",
        "opt = config()\n",
        "seed_everything(opt.seed)"
      ],
      "metadata": {
        "id": "j_1JreZf-IV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "config.modelUNet.params.ddim_steps = opt.ddim_steps\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "\n",
        "if opt.precision == \"autocast\":\n",
        "    model.half()\n",
        "    modelCS.half()\n",
        "    modelFS.half()"
      ],
      "metadata": {
        "id": "i50YPZRo-IRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Prompt"
      ],
      "metadata": {
        "id": "u1lfVBO69tTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "add \"../\" before the name for images in the root gdrive(MyDrive) folder\n",
        "\n",
        "this video(not mine) shows how to upload images: https://siasky.net/_ABvKVbl9c9FDFywm4HauIr3g2EgPg2YbRE8burq8IsK-w\n",
        "\n",
        "strength sets how agressive it is when modifying the images"
      ],
      "metadata": {
        "id": "jttBX5SNP0cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\" #@param {type:\"string\"}\n",
        "img = \"../visage.jpg\" #@param {type:\"string\"}\n",
        "scale = 5 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 100 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "strength = 0.6 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "n_rows = 2 #@param {type:\"integer\"}\n",
        "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
        "opt.scale = scale\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "opt.strength = strength\n",
        "images = generate(opt, init_img=img, grid=(grid==\"yes\"), prompt=prompt)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "08WwrY7e-IzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "wDFl3Ndu9tHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you generated more than 5 images(the grid counts too), just add more code lines continuing the sequence\n",
        "\n",
        "If you chose grid, it's the first one\n",
        "\n",
        "If your outputs are a colourful rainbow mess, you need to enlarge the input image\n",
        "\n",
        "Right click and save to download"
      ],
      "metadata": {
        "id": "OEY8xGg0-UKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "FdSHWaCn-VhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "GBzcMkcR-VXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "7_bPRoFo-VSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "D9BXui3g-VMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "fwO3ST0i-UxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}